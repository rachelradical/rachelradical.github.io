{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "#import libraries and load dataset\n",
        "\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from scipy import stats\n",
        "\n",
        "# Load the dataset\n",
        "df = pd.read_csv('../data/barcode_scans.csv')\n",
        "\n",
        "# Convert date column to datetime for easier analysis\n",
        "df['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Display first few rows to confirm structure\n",
        "df.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "id": "d5tyRALIJXWU",
        "outputId": "835d79eb-3425-4a65-e65a-27d26873b580"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "error",
          "ename": "FileNotFoundError",
          "evalue": "[Errno 2] No such file or directory: '../data/barcode_scans.csv'",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-1-e471db96a2b0>\u001b[0m in \u001b[0;36m<cell line: 0>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;31m# Load the dataset\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 9\u001b[0;31m \u001b[0mdf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpd\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread_csv\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'../data/barcode_scans.csv'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     10\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     11\u001b[0m \u001b[0;31m# Convert date column to datetime for easier analysis\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36mread_csv\u001b[0;34m(filepath_or_buffer, sep, delimiter, header, names, index_col, usecols, dtype, engine, converters, true_values, false_values, skipinitialspace, skiprows, skipfooter, nrows, na_values, keep_default_na, na_filter, verbose, skip_blank_lines, parse_dates, infer_datetime_format, keep_date_col, date_parser, date_format, dayfirst, cache_dates, iterator, chunksize, compression, thousands, decimal, lineterminator, quotechar, quoting, doublequote, escapechar, comment, encoding, encoding_errors, dialect, on_bad_lines, delim_whitespace, low_memory, memory_map, float_precision, storage_options, dtype_backend)\u001b[0m\n\u001b[1;32m   1024\u001b[0m     \u001b[0mkwds\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mupdate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mkwds_defaults\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1025\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1026\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0m_read\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1027\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1028\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_read\u001b[0;34m(filepath_or_buffer, kwds)\u001b[0m\n\u001b[1;32m    618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    619\u001b[0m     \u001b[0;31m# Create the parser.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 620\u001b[0;31m     \u001b[0mparser\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTextFileReader\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfilepath_or_buffer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwds\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    622\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mchunksize\u001b[0m \u001b[0;32mor\u001b[0m \u001b[0miterator\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, f, engine, **kwds)\u001b[0m\n\u001b[1;32m   1618\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1619\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mhandles\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mIOHandles\u001b[0m \u001b[0;34m|\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1620\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_engine\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_make_engine\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mengine\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1621\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1622\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mclose\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m->\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/parsers/readers.py\u001b[0m in \u001b[0;36m_make_engine\u001b[0;34m(self, f, engine)\u001b[0m\n\u001b[1;32m   1878\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1879\u001b[0m                     \u001b[0mmode\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;34m\"b\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1880\u001b[0;31m             self.handles = get_handle(\n\u001b[0m\u001b[1;32m   1881\u001b[0m                 \u001b[0mf\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1882\u001b[0m                 \u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.11/dist-packages/pandas/io/common.py\u001b[0m in \u001b[0;36mget_handle\u001b[0;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors, storage_options)\u001b[0m\n\u001b[1;32m    871\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mencoding\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0;34m\"b\"\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    872\u001b[0m             \u001b[0;31m# Encoding\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 873\u001b[0;31m             handle = open(\n\u001b[0m\u001b[1;32m    874\u001b[0m                 \u001b[0mhandle\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    875\u001b[0m                 \u001b[0mioargs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: '../data/barcode_scans.csv'"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#detects frequency-based anomalies\n",
        "# Calculate Z-scores for scan counts\n",
        "df['z_score'] = stats.zscore(df['scan_count'])\n",
        "\n",
        "# Define anomaly threshold (anything >3 or <-3 is unusual)\n",
        "df['anomaly'] = df['z_score'].apply(lambda x: abs(x) > 3)\n",
        "\n",
        "# Display detected anomalies\n",
        "anomalies = df[df['anomaly']]\n",
        "print(f\"âœ… Detected {len(anomalies)} anomalies!\")\n",
        "print(anomalies.head())\n"
      ],
      "metadata": {
        "id": "Q9JszKpMiz-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to validate ID checksum based on state rules\n",
        "def validate_checksum(id_number, state):\n",
        "    try:\n",
        "        id_digits = [int(digit) for digit in str(id_number)]\n",
        "        if len(id_digits) != 8:\n",
        "            return False  # ID should be 8 digits long\n",
        "\n",
        "        if state == \"PA\":  # Pittsburgh & Philadelphia\n",
        "            return id_digits[0] + id_digits[7] == 11\n",
        "        elif state == \"IL\":  # Chicago\n",
        "            return (id_digits[0] * id_digits[3]) % 2 == 0\n",
        "        else:\n",
        "            return True  # Default to valid if unknown state\n",
        "    except:\n",
        "        return False  # Any error means invalid ID\n",
        "\n",
        "# Apply validation function to dataset\n",
        "df['calculated_valid_id'] = df.apply(lambda row: validate_checksum(row['id_number'], row['state']), axis=1)\n",
        "\n",
        "# Flag invalid IDs\n",
        "df['invalid_id_anomaly'] = df['calculated_valid_id'] != True\n",
        "\n",
        "# Show detected invalid IDs\n",
        "invalid_ids = df[df[\"invalid_id_anomaly\"]]\n",
        "print(f\"âœ… Detected {len(invalid_ids)} invalid ID anomalies.\")\n",
        "invalid_ids[['date', 'location', 'state', 'store_name', 'id_number']].head()\n"
      ],
      "metadata": {
        "id": "gD86iMkILC44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUoDts7q6Gw2"
      },
      "outputs": [],
      "source": [
        "# Sort dataset by time for consecutive anomaly detection\n",
        "df = df.sort_values(by=['date', 'hour'])\n",
        "\n",
        "# Create a column to track consecutive invalid IDs\n",
        "df[\"consecutive_invalid_id\"] = (df[\"invalid_id_anomaly\"] & df[\"invalid_id_anomaly\"].shift(1))\n",
        "\n",
        "# Show detected consecutive anomalies\n",
        "consecutive_anomalies = df[df[\"consecutive_invalid_id\"]]\n",
        "print(f\"âœ… Detected {len(consecutive_anomalies)} consecutive invalid ID anomalies.\")\n",
        "consecutive_anomalies[['date', 'location', 'store_name', 'id_number']].head()\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate overall anomaly rates\n",
        "total_scans = df.shape[0]\n",
        "total_anomalies = df['invalid_id_anomaly'].sum()\n",
        "average_anomaly_rate = total_anomalies / total_scans\n",
        "\n",
        "# Calculate anomaly rate per store\n",
        "store_anomaly_rates = df.groupby(\"store_name\")['invalid_id_anomaly'].mean().reset_index()\n",
        "store_anomaly_rates.columns = [\"store_name\", \"anomaly_rate\"]\n",
        "\n",
        "# Flag stores that have **1.5x the average anomaly rate**\n",
        "store_anomaly_rates[\"store_anomaly\"] = store_anomaly_rates[\"anomaly_rate\"] > (1.5 * average_anomaly_rate)\n",
        "\n",
        "# Show stores with high anomaly rates\n",
        "high_anomaly_stores = store_anomaly_rates[store_anomaly_rates[\"store_anomaly\"]]\n",
        "print(f\"âœ… Stores with high anomaly rates ({len(high_anomaly_stores)} detected):\")\n",
        "high_anomaly_stores\n"
      ],
      "metadata": {
        "id": "GT8s8yMOLMdG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "plt.figure(figsize=(12, 6))\n",
        "plt.bar(store_anomaly_rates[\"store_name\"], store_anomaly_rates[\"anomaly_rate\"],\n",
        "        color=[\"red\" if x else \"blue\" for x in store_anomaly_rates[\"store_anomaly\"]])\n",
        "plt.axhline(y=average_anomaly_rate, color='black', linestyle='dashed', label=\"Average Anomaly Rate\")\n",
        "plt.xlabel(\"Store Name\")\n",
        "plt.ylabel(\"Anomaly Rate\")\n",
        "plt.title(\"Store Anomaly Rates\")\n",
        "plt.xticks(rotation=45)\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "\n",
        "# Plot the histogram of scan counts\n",
        "plt.hist(df[\"scan_count\"], bins=50, alpha=0.7, label=\"Scan Counts\")\n",
        "\n",
        "# Mark the mean scan count with a dashed line\n",
        "plt.axvline(df[\"scan_count\"].mean(), color=\"black\", linestyle=\"dashed\", linewidth=2, label=\"Mean Scan Count\")\n",
        "\n",
        "# Highlight potential scan anomalies (Z-score > 3)\n",
        "anomalous_scans = df[df[\"scan_anomaly\"]][\"scan_count\"]\n",
        "if not anomalous_scans.empty:\n",
        "    plt.hist(anomalous_scans, bins=50, color=\"red\", alpha=0.6, label=\"Anomalous Scans\")\n",
        "\n",
        "plt.xlabel(\"Number of Scans\")\n",
        "plt.ylabel(\"Frequency\")\n",
        "plt.title(\"Distribution of Scan Counts (with Anomaly Detection)\")\n",
        "plt.legend()\n",
        "plt.show()\n",
        "\n"
      ],
      "metadata": {
        "id": "QoIr1qrQLV44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df.to_csv(\"../data/detected_anomalies.csv\", index=False)\n",
        "print(\"âœ… Anomaly detection complete! Data saved for storytelling.\")"
      ],
      "metadata": {
        "id": "gZBZwUbtn3tz"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}