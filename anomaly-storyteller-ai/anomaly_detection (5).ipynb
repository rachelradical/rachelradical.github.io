{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "from scipy import stats\n",
        "\n",
        "# Load the dataset\n",
        "data_dir = \"data\"\n",
        "df = pd.read_csv(os.path.join(data_dir, \"barcode_scans.csv\"))\n",
        "\n",
        "print(\"Dataset loaded successfully!\")\n",
        "df.drop('is_invalid_id', axis=1, inplace=True)\n",
        "df.head()\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "d5tyRALIJXWU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Function to validate ID checksum based on state rules\n",
        "def validate_checksum(id_number, state):\n",
        "    try:\n",
        "        id_digits = [int(digit) for digit in str(id_number)]\n",
        "        if len(id_digits) != 8:\n",
        "            return False  # ID should be 8 digits long\n",
        "\n",
        "        if state == \"PA\":  # Pittsburgh & Philadelphia\n",
        "            return id_digits[0] + id_digits[7] == 11\n",
        "        elif state == \"IL\":  # Chicago\n",
        "            return (id_digits[0] * id_digits[3]) % 2 == 0\n",
        "        else:\n",
        "            return True  # Default to valid if unknown state\n",
        "    except:\n",
        "        return False  # Any error means invalid ID\n",
        "\n",
        "# Apply validation function to dataset\n",
        "df['calculated_valid_id'] = df.apply(lambda row: validate_checksum(row['id_number'], row['state']), axis=1)\n",
        "\n",
        "# Flag invalid IDs\n",
        "df['checksum_anomalies'] = df['calculated_valid_id'] == False\n",
        "\n",
        "# Show detected invalid IDs\n",
        "checksum_anomalies = df[df[\"checksum_anomalies\"]]\n",
        "print(f\"‚úÖ Detected {len(checksum_anomalies)} checksum anomalies.\")\n"
      ],
      "metadata": {
        "id": "Q9JszKpMiz-m"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Calculate Z-scores for scan counts\n",
        "df['scan_count_z_score'] = stats.zscore(df['scan_count'])\n",
        "\n",
        "# Define anomaly threshold\n",
        "df['scan_count_anomaly'] = df['scan_count_z_score'].apply(lambda x: abs(x) > 3.5)\n",
        "\n",
        "# Display detected anomalies\n",
        "scan_count_anomalies = df[df['scan_count_anomaly']]\n",
        "print(f\"‚úÖ Detected {len(scan_count_anomalies)} total scan anomalies.\")\n",
        "\n",
        "\n",
        "scan_count_anomalies.to_csv(os.path.join(data_dir, \"scan_count_anomalies\"), index=False)\n",
        "scan_count_anomalies.head()\n",
        "\n"
      ],
      "metadata": {
        "id": "gD86iMkILC44"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LUoDts7q6Gw2"
      },
      "outputs": [],
      "source": [
        "# Merge Anomalies into One Unified DataFrame\n",
        "overall_anomalies = pd.concat([checksum_anomalies, scan_count_anomalies], ignore_index=True)\n",
        "\n",
        "# Remove duplicates to avoid counting the same anomaly multiple times\n",
        "overall_anomalies.drop_duplicates(subset=[\"date\", \"hour\", \"location\", \"state\", \"store_name\", \"device_type\", \"id_number\"], inplace=True)\n",
        "\n",
        "# Save the Merged Anomalies Dataset\n",
        "merged_anomalies_path = os.path.join(data_dir, \"merged_anomalies.csv\")\n",
        "overall_anomalies.to_csv(merged_anomalies_path, index=False)\n",
        "\n",
        "print(f\"‚úÖ Merged {len(overall_anomalies)} anomalies into {merged_anomalies_path}.\")\n",
        "print(\"‚úÖ Anomaly detection complete!")"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "#Prepare anomaly data for storyteller AI\n",
        "\n",
        "data_dir = \"data\"\n",
        "merged_anomalies_path = os.path.join(data_dir, \"merged_anomalies.csv\")\n",
        "storyteller_data_path = os.path.join(data_dir, \"storyteller_data.json\")\n",
        "\n",
        "if os.path.exists(merged_anomalies_path):\n",
        "    df = pd.read_csv(merged_anomalies_path)\n",
        "    df.to_json(storyteller_data_path, orient=\"records\", indent=4)\n",
        "    print(f\"‚úÖ storyteller_data.json successfully created at {storyteller_data_path}.\")\n",
        "else:\n",
        "    print(\"‚ùå merged_anomalies.csv not found. Please run anomaly_detection.ipynb to generate it first.\")\n"
      ],
      "metadata": {
        "id": "mXbBRntNggQl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "# üîπ Step 1: Load the Merged Anomalies Dataset\n",
        "data_dir = \"data\"\n",
        "merged_anomalies = pd.read_csv(os.path.join(data_dir, \"barcode_scans.csv\"))\n",
        "\n",
        "merged_anomalies['date'] = pd.to_datetime(df['date'])\n",
        "\n",
        "# Filter for checksum anomalies\n",
        "checksum_anomalies_df = df[df['checksum_anomalies'] == True]\n",
        "scancount_anomalies_df = df[df['scan_count_anomaly'] == True]\n",
        "\n",
        "# Group by date and location, then count anomalies\n",
        "anomaly_counts = checksum_anomalies_df.groupby(['date', 'location']).size().unstack(fill_value=0)\n",
        "scancounts = scancount_anomalies_df.groupby(['date', 'location']).size().unstack(fill_value=0)\n",
        "# Plotting\n",
        "plt.figure(figsize=(12, 6))\n",
        "for location in anomaly_counts.columns:\n",
        "    plt.plot(anomaly_counts.index, anomaly_counts[location], label=location, marker='o')\n",
        "\n",
        "plt.title('Checksum Anomalies Over Time by Location')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Checksum Anomalies')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "plt.figure(figsize=(12, 6))\n",
        "for location in scancounts.columns:\n",
        "    plt.plot(scancounts.index, scancounts[location], label=location, marker='o')\n",
        "\n",
        "plt.title('Repeated Scan Anomalies Over Time by Location')\n",
        "plt.xlabel('Date')\n",
        "plt.ylabel('Number of Repeated Scan Anomalies')\n",
        "plt.legend()\n",
        "plt.grid(True)\n",
        "plt.tight_layout()\n",
        "plt.show()\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "GT8s8yMOLMdG"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}
